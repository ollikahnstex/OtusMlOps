# Отчёт по ДЗ №3

## Очистка данных
В рамках ДЗ в Yandex Cloud Object Storage был создан новый bucket:  
**https://storage.yandexcloud.net/otus-mlops-dproc**.  
![Новый bucket](hw3_img/new_bucket.PNG "Новый bucket")  

Был создан новый Spark-кластер с привязкой к данному bucket'у.  

![Новый Spark-кластер](hw3_img/new_spark_cluster.PNG "Новый Spark-кластер")  
Соответствующим образом была настроена группа безопасности.  
![Настройка группы безопасности](hw3_img/cluster_secgroup_in.PNG "Настройка группы безопасности")  
![Настройка группы безопасности](hw3_img/cluster_secgroup_out.PNG "Настройка группы безопасности")  

Для анализа данных использовал Jupyter Notebook на мастер-ноде кластера. Создал виртуальное окружение,
подключил его к Kernel'ам, создал сессию подключения к s3-хранилищу с данными по транзакциям.  
![Jupyter Notebook а Spark-кластере](hw3_img/jupyter_on_spark_cluster.PNG "Jupyter Notebook а Spark-кластере")  

Данные из s3-хранилища небольшим объёмом (тысяч по 10-50 строк из пары файлов) шли быстро (пара минут),
а вот объёме 10 млн. строк уже выгружались заметно дольше.  
![Выгрузка данных из объектного хранилища](hw3_img/data_from_s3.PNG "Выгрузка данных из объектного хранилища")  
По каким-то причинам периодически подключение к кластеру терялось, и kernel вырубался, а соответственно
данные выпадали из оператвной памяти. Много времени на это ушло, поэтому решил всё-таки перенести данные
на Spark-кластер на время обработки по аналогии с ДЗ №2 (возможно, так и подразумевалось изначально).  

Обработка данных шла крайне долго: фильтрации и/или агрегации занимали 5-15 минут. В связи с этим
было решено для составления скрипта вести анализ только на одном файле датасета.  
![Файл примера данных](hw3_img/example_data.PNG "Файл примера данных")  
![Начало_анализа](hw3_img/example_data_analysis.PNG "Начало_анализа")  

Jupyter Notebook с тестовой обработкой данных:
[data_cleaning.ipynb](data_cleaning.ipynb)

После отладки кода на тестовом примере был написан python-скрипт обработки данных на PySpark:
[data_cleaning.py](../data_cleaning.py)

Данный скрипт принимает на вход путь до файла на HDFS Spark-кластера, чистит файл и сохраняет его в формате
`parquet` на s3-хранилище в ранее созданном бакете **https://storage.yandexcloud.net/otus-mlops-dproc**.
Для запуска этого скрипта на каждом файле с транзакциями из HDFS я выполнили bash-команду, которая
составляет список путей до всех файлов `.txt` на HDFS и передаёт их имена в python-скрипт.
```bash
hdfs dfs -ls /user/ubuntu/fraud_data | grep .txt$ | awk '{print $8}' | while read fn; do python ../data_cleaning.py $fn; done
```
![Работа скрипта](hw3_img/process_all_data.PNG "Работа скрипта")  
